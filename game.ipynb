{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d20c0ce6-a68a-4a51-b6ad-7976b147fdf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, glob, os\n",
    "from termcolor import colored\n",
    "import math\n",
    "from datetime import datetime\n",
    "import h5py                         # pip install h5py\n",
    "import pywt                         # pip install PyWavelets\n",
    "from tqdm import tqdm               # pip install tqdm\n",
    "import matplotlib.pyplot as plt     # pip install matplotlib\n",
    "import cv2                          # pip install opencv-python\n",
    "import numpy as np                  # pip install numpy\n",
    "from prettytable import PrettyTable # pip install prettytable\n",
    "\n",
    "# for torch installation, follow https://pytorch.org/get-started/locally/ or https://pytorch.org/get-started/previous-versions/\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b99a6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.common import get_save_path, get_load_path, normalize_dataset\n",
    "from utils.save_load_model import save_model, load_model\n",
    "from utils.metrics import get_CM, get_AUROC, get_IoU, compute_metrics, nice_print, compute_avg_metrics, compute_best_classifier\n",
    "from utils.matlab_imresize import imresize\n",
    "from utils.model import Model, create_model\n",
    "from utils.classifier import train_classifier, test_classifier\n",
    "from utils.log_funcs import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "503eeac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "####### PARAMETERS #######\n",
    "debug_mode = True\n",
    "\n",
    "wavelet_type = \"DWT\" # \"DWT\" or \"CWT\"\n",
    "\n",
    "dataset = \"BBB\" # \"ACPdata\" or \"BBB\"\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "out_path = f\"model_checkpoints_{dataset}_{wavelet_type}_{timestamp}\"\n",
    "\n",
    "siz = 224  # size of the image (siz x siz x 3) to feed to resnet model\n",
    "\n",
    "classifiers_num = 1\n",
    "\n",
    "max_epochs = 2\n",
    "\n",
    "train_batch_size = 32\n",
    "\n",
    "initial_learning_rate = 1e-4\n",
    "scheduler_rate = 0.8\n",
    "lr_last_fc_weight = 20\n",
    "\n",
    "# The following wavelets are also available: 'dmey', 'fk4', 'beyl', 'vaid'\n",
    "# Consult the PyWavelets documentation for a complete list of available wavelets\n",
    "if wavelet_type==\"DWT\":\n",
    "    wavelets = np.array(['haar', 'db6', 'sym6', 'coif2', 'bior2.2', 'rbio2.2'])\n",
    "elif wavelet_type==\"CWT\":\n",
    "    wavelets = np.array(['cgau6', 'cmor', 'fbsp', 'gaus6', 'mexh', 'morl', 'shan'])\n",
    "else:\n",
    "    quit(\"Invalid wavelet type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d92d7b",
   "metadata": {},
   "source": [
    "## Loading Features data\n",
    "\n",
    "Here are the data vectors that are going to be transformed into 3-channel matrices.\n",
    "\n",
    "Then, a model must learn how to classify each vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f05eac84-7ecc-4c25-9b1b-d72cdaeac515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opens an HDF5 file and retrieves its items.\n",
    "# variables (ItemsView): The items in the HDF5 file.\n",
    "if dataset==\"BBB\":\n",
    "    fBBB = h5py.File('datasets/BBBfold.mat','r')\n",
    "elif dataset==\"ACPdata\":\n",
    "    fBBB = h5py.File('datasets/ACPdata.mat','r')\n",
    "else:\n",
    "    quit(\"Invalid dataset\")\n",
    "variables = fBBB.items()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5eecbd-58ca-48ad-9362-3a0e547f1072",
   "metadata": {},
   "source": [
    "### Notes for the BBB dataset\n",
    "data size is: C x N = 1119 x 7162 (where C is the number of channels and N is the number of data samples) <br>\n",
    "we have N=7162 samples, each has C=1119 features. <br>\n",
    "This dataset is divided 10 folds, each using 6446 samples for train and 716 for validation\n",
    " - 6446 + 716 = 7162 (~90% for train and ~10% for valid)<br>\n",
    "\n",
    "The test set (referred to as dataIND and labels labelIND)  has size 1119x74, that is 74 samples.<br>\n",
    "\n",
    "The next cell prints all the variables in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6086095a-ef3c-4558-b844-e5d7e5e76236",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_s = max([len(var[0]) for var in variables])\n",
    "for var in variables:\n",
    "    name = var[0]\n",
    "    data = np.array(fBBB.get(name))\n",
    "    print(f\"{name} {' '*(max_s-len(name))} {data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c74be71-337a-43bb-be9b-328cf079afc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data     = np.array(fBBB.get(\"data\")).T ## e.g. for BBB: shape: (1119, 7162) to (7162, 1119) with transposition\n",
    "numFolds = np.array(fBBB.get(\"idTR\")).shape[0]\n",
    "print(\"numFolds\", numFolds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c1db97-6c44-4f10-bdeb-53c6d3ffc299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle features for each channel (3 in total)\n",
    "NumberOfFeatures = data.shape[1]\n",
    "print(\"NumberOfFeatures\", NumberOfFeatures)\n",
    "np.random.seed(1337)\n",
    "# ord = {canale: np.random.permutation(NumberOfFeatures) for canale in range(3)}\n",
    "ord = {canale: np.argsort(np.random.random(NumberOfFeatures)) for canale in range(3)}\n",
    "print(f\"ord is a dict with {len(ord.items())} items, each has len {ord[0].shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58123155-6369-4c29-80b8-b41de5dba9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly select mother wavelets for each band (3 bands in total)\n",
    "np.random.seed(1337)\n",
    "SelectedWave = {}\n",
    "for band in range(3):\n",
    "    Selected = []\n",
    "    for choose in range(len(wavelets)):\n",
    "        # Randomly shuffle wavelets and store the indices\n",
    "        SW = np.argsort(np.random.random(len(wavelets)))\n",
    "        Selected.append(SW)\n",
    "    SelectedWave[band] = np.array(Selected).reshape(-1)\n",
    "print(\"SelectedWave:\", type(SelectedWave), \"len:\", len(SelectedWave), \"each band has shape\", SelectedWave[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf105d6",
   "metadata": {},
   "source": [
    "# vector2image function\n",
    "HERE is where you have to modify the code. Choose btw vector2image_DWT and vector2image_CWT.\n",
    " > Just one is sufficient\n",
    "\n",
    "You already find the baseline version of the functions.\n",
    " > Play with them to gain confidence with the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db61937b-695e-4304-b366-8c52e1e5808d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector2image_DWT(network_features, L=2):\n",
    "\n",
    "    long = network_features.shape[0]>500\n",
    "\n",
    "    IM = []\n",
    "    for band in range(3):\n",
    "        ordband = ord[band]\n",
    "        \n",
    "        # Reorder data sample (aka network_features) using the shuffling indexes ordband\n",
    "        vector = network_features[ordband]\n",
    "\n",
    "        # Initialize the matrix with zeros\n",
    "        N = int(np.ceil(vector.shape[0]/(2*L)))\n",
    "        N_safe = N + 200\n",
    "        \n",
    "        matrix = np.zeros((N_safe, N_safe)); ## to account for the long vectors ca/cd (\"avoid lossy\")\n",
    "        \n",
    "        num_levels = int(np.log2(len(vector)))\n",
    "\n",
    "        # wavelet sets extraction\n",
    "        row = 0\n",
    "        OrigVector = vector.copy()\n",
    "        esco = False\n",
    "        NumWave = len(SelectedWave[band])\n",
    "        diffWavelet = 1\n",
    "\n",
    "        max_ca = -1\n",
    "        \n",
    "        while True:\n",
    "            vector=OrigVector.copy()\n",
    "    \n",
    "            # Fill the matrix with the wavelet coefficients\n",
    "            for i in range(num_levels-4):\n",
    "                wavelet_index = SelectedWave[band][diffWavelet % NumWave]\n",
    "                wavelet_type = wavelets[wavelet_index]\n",
    "                \n",
    "                ca, cd = pywt.dwt(vector, wavelet_type, mode=\"symmetric\")\n",
    "                \n",
    "                if long and i==0:\n",
    "                    ca = imresize(ca[None].T, output_shape=(np.ceil(ca.shape[0]/2), 1))[:, 0]\n",
    "                    cd = imresize(cd[None].T, output_shape=(np.ceil(cd.shape[0]/2), 1))[:, 0]\n",
    "                    \n",
    "                vector = ca\n",
    "\n",
    "                if len(ca)>max_ca:\n",
    "                    max_ca = len(ca)\n",
    "\n",
    "                matrix[row,   :ca.shape[0]] = ca\n",
    "                matrix[row+1, :cd.shape[0]] = cd\n",
    "                row += 2\n",
    "                \n",
    "                if row>max_ca:\n",
    "                    esco=True\n",
    "                    break\n",
    "            \n",
    "            if esco:\n",
    "                break\n",
    "            diffWavelet += 1\n",
    "\n",
    "        matrix = matrix[:row, :max_ca]\n",
    "        IM.append(matrix)\n",
    "    \n",
    "    IMA = [imresize(IM[i], output_shape=(224, 224)) for i in range(len(IM))]\n",
    "    IM = np.array(IMA)\n",
    "    return IM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15984c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector2image_CWT(network_features, L=2):\n",
    "    New_IM = []\n",
    "\n",
    "    for band in range(3):\n",
    "        ordband = ord[band]\n",
    "        \n",
    "        # Sample vector of length 'n'\n",
    "        ## OR: reorder data sample (aka network_features) using the shuffling indexes ordband\n",
    "        vector = network_features[ordband]\n",
    "        sizeFeat = vector.shape[0]\n",
    "        \n",
    "        # we divide the feature vector in sub-windows of size 1% of the total length\n",
    "        BL = np.ceil(sizeFeat/100)\n",
    "        IM = []\n",
    "        for sottoM in range(0, sizeFeat, int(BL)):\n",
    "            net_feat = network_features[sottoM:min(sottoM + int(BL), sizeFeat)]  # sub-window\n",
    "\n",
    "            IM.append(pywt.cwt(net_feat, np.arange(1, len(net_feat)+1), wavelets[SelectedWave[band][0]])[0])  # Continuous wavelet transform\n",
    "\n",
    "        # it stores the matrix obtained concatenating the 100 outputs of the above CWT\n",
    "        IMM = []\n",
    "        step_idx = 0\n",
    "        for row in range(10):\n",
    "            IMMcl=[]\n",
    "            for col in range(10):\n",
    "                try:\n",
    "                    Blocco = IM[step_idx]\n",
    "                except IndexError:\n",
    "                    Blocco = np.zeros((IM[0].shape[0], IM[0].shape[1]))\n",
    "                if Blocco.shape[0] < IM[0].shape[0] or Blocco.shape[1] < IM[0].shape[1]:  # padding for the right size\n",
    "                    padded_Blocco = np.zeros((IM[0].shape[0], IM[0].shape[1]))\n",
    "                    padded_Blocco[:Blocco.shape[0], :Blocco.shape[1]] = Blocco\n",
    "                    Blocco = padded_Blocco\n",
    "                IMMcl.append(Blocco)\n",
    "                step_idx += 1\n",
    "            \n",
    "            IMMcl = np.concatenate(IMMcl, axis=1)\n",
    "\n",
    "            IMM.append(IMMcl)\n",
    "        \n",
    "        IMM = np.concatenate(IMM, axis=0)\n",
    "    \n",
    "        New_IM.append(IMM)\n",
    "\n",
    "    # Resize the image to 224x224\n",
    "    IMA = [imresize(New_IM[i], output_shape=(224, 224)) for i in range(len(New_IM))]\n",
    "    IM = np.array(IMA)\n",
    "    return IM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6917f9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if wavelet_type==\"CWT\":\n",
    "    vector2image = vector2image_CWT\n",
    "elif wavelet_type==\"DWT\":\n",
    "    vector2image = vector2image_DWT\n",
    "else:\n",
    "    quit(\"Invalid wavelet type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6b0bf1",
   "metadata": {},
   "source": [
    "## Visualize an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11f568f-abd6-41a4-abd9-c8a49f7f996f",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_data = data[:100, :]\n",
    "\n",
    "# 1. normalize data\n",
    "norm_data, _, _ = normalize_dataset(subset_data)\n",
    "\n",
    "# 2. pick a vector at random and convert it to an image\n",
    "idx = np.random.randint(0, norm_data.shape[0])\n",
    "IM = vector2image(norm_data[idx])\n",
    "\n",
    "# # 3. imshow wants the channel dimension to be the last one\n",
    "IM = np.transpose(IM, (1, 2, 0))\n",
    "\n",
    "# # 4. normalize the image to [0, 1] range for visualization\n",
    "IMA = ((IM - IM.min()) / (IM.max() - IM.min()))\n",
    "\n",
    "plt.imshow(IMA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549e3df5-500f-4f16-87a4-274d4c80341a",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97e4d33",
   "metadata": {},
   "source": [
    "### GenerativeDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a4f0c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerativeDataset(Dataset):\n",
    "    def __init__(self, fBBB, fold, mode=\"TR\", siz=224, normalization=None, downsize=False):\n",
    "       \n",
    "        self.siz = siz\n",
    "\n",
    "        if dataset == \"BBB\":\n",
    "            keys = {\"train\": [\"data\", \"label\"], \"test\": [\"dataIND\", \"labelIND\"], \"valid\": [\"data\", \"label\"]}\n",
    "        elif dataset==\"ACPdata\":\n",
    "            keys = {\"train\": [\"FeatureTR\", \"labelTR\"], \"test\": [\"FeatureTE\", \"labelTE\"], \"valid\": [\"FeatureTE\", \"labelTE\"]}\n",
    "        \n",
    "        if fold is not None:\n",
    "            data = np.array(fBBB.get(\"data\")).T\n",
    "\n",
    "            self.idxs = np.array(fBBB.get(fBBB.get(f\"id{mode}\")[fold, 0]))[0].astype(int) -1\n",
    "\n",
    "            print(\"creating dataset with mode\", mode, \"fold\", fold)\n",
    "\n",
    "            ## reduce the dataset sample in favor of speed\n",
    "            ## set this option in prototyping phase, convergence should happen\n",
    "            if downsize:\n",
    "                self.idxs_t = [self.idxs[idx] for idx in range(len(self.idxs)) if idx%10==0]\n",
    "                print(\"downsizing from\", len(self.idxs), \"to\", len(self.idxs_t))\n",
    "                self.idxs = np.array(self.idxs_t)\n",
    "            \n",
    "            self.vectors = data[self.idxs, :]\n",
    "            if dataset==\"BBB\":\n",
    "                self.labels = np.array(fBBB.get(\"label\"))[:, 0][self.idxs].astype(int) -1\n",
    "            else:\n",
    "                self.labels = np.array(fBBB.get(np.array(fBBB.get(keys[mode][1])[0])[0]))[0].astype(int) - 1\n",
    "        else:\n",
    "            \n",
    "            self.vectors = np.array(fBBB.get(keys[mode][0])).T\n",
    "            \n",
    "            if dataset==\"BBB\":\n",
    "                self.labels = np.array(fBBB.get(keys[mode][1]))[:, 0].astype(int) -1\n",
    "            else:\n",
    "                self.labels = np.array(fBBB.get(np.array(fBBB.get(keys[mode][1])[0])[0]))[0].astype(int) - 1\n",
    "\n",
    "            if downsize:\n",
    "                self.idxs = np.arange(0, self.vectors.shape[0])\n",
    "                np.random.seed(1337); np.random.shuffle(self.idxs)\n",
    "                self.idxs_t = [self.idxs[idx] for idx in range(len(self.idxs)) if idx%10==0]\n",
    "                print(\"downsizing from\", len(self.idxs), \"to\", len(self.idxs_t))\n",
    "                self.idxs = np.array(self.idxs_t)\n",
    "                self.vectors = self.vectors[self.idxs, :]\n",
    "                self.labels = self.labels[self.idxs].astype(int)\n",
    "\n",
    "            if mode==\"valid\":\n",
    "                self.idxs = np.arange(0, self.vectors.shape[0])\n",
    "                np.random.seed(1337)\n",
    "                np.random.shuffle(self.idxs)\n",
    "                self.idxs = self.idxs[:int(0.2 * len(self.idxs))]\n",
    "                self.vectors = self.vectors[self.idxs, :]\n",
    "                self.labels = self.labels[self.idxs].astype(int)\n",
    "\n",
    "        assert self.vectors.shape[0] == self.labels.shape[0], str(self.vectors.shape[0]) + \" and \" + str(self.labels.shape[0]) + \" don't match!\"\n",
    "\n",
    "        if normalization is None:\n",
    "            self.vectors, min_, max_ = normalize_dataset(self.vectors)\n",
    "            self.min_ = min_\n",
    "            self.max_ = max_\n",
    "        else:\n",
    "            self.vectors, _, _ = normalize_dataset(self.vectors, min_along_cols=normalization[0], max_along_cols=normalization[1])\n",
    "\n",
    "        ## compute class weights to help the Loss function to stabilize\n",
    "        ## since the dataset is heavily unbalanced\n",
    "        self.w_0 = self.labels.shape[0] / (2 * (self.labels==0).sum())\n",
    "        self.w_1 = self.labels.shape[0] / (2 * (self.labels==1).sum())\n",
    "        \n",
    "        print(\"negatives:\", (self.labels==0).sum(), \"/\", self.labels.shape[0], \"w_0\", self.w_0)\n",
    "        print(\"positives:\", (self.labels==1).sum(), \"/\", self.labels.shape[0], \"w_1\", self.w_1)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.vectors)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        network_features=self.vectors[idx, :]\n",
    "        IM = vector2image(network_features)\n",
    "        IM = np.transpose(IM, (1, 2, 0))\n",
    "        lab = self.labels[idx]\n",
    "        return IM, lab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738e5e55-2da7-49e9-8f0c-08996c7f013b",
   "metadata": {},
   "source": [
    "## Normalize the \"images\" to mean 0 and variance 1\n",
    "As you studied in the course, this helps the network to learn and speeds up convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1a289f1-3701-4001-8e74-975a26af20f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mean_std(loader):\n",
    "    mean = std = total_images = 0\n",
    "    \n",
    "    for images, _ in tqdm(loader):\n",
    "        # Permute images to [N, 3, 224, 224] and reshape to [N, 3, -1] for easier mean/std calculation\n",
    "        images = images.permute(0, 3, 1, 2)\n",
    "        images = images.view(images.size(0), images.size(1), -1)\n",
    "        mean  += images.mean(2).sum(0)\n",
    "        std   += images.std(2).sum(0)\n",
    "        total_images += images.size(0)\n",
    "\n",
    "    mean /= total_images\n",
    "    std /= total_images\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fc067f-e228-4cf0-89da-da0975288eef",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b949a0",
   "metadata": {},
   "source": [
    "## (only for BBB) Cross-Validation Training on all the folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6498b498",
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset == \"ACPdata\":\n",
    "    print(\"do not train with cross validation on ACPdata\")\n",
    "    print(\"you're only given a single fold\")\n",
    "else:\n",
    "    classifier_idx = 0  ## modify this to save different classifiers test results\n",
    "\n",
    "    results = []\n",
    "    # numFolds = 2\n",
    "    for fold_idx in range(numFolds):\n",
    "        print(colored(f\"starting fold {fold_idx} / {numFolds}\", \"yellow\"))\n",
    "        trainset = GenerativeDataset(fBBB, fold=fold_idx, mode=\"TR\", normalization=None, downsize=False)\n",
    "        validset = GenerativeDataset(fBBB, fold=fold_idx, mode=\"TE\", normalization=(trainset.min_, trainset.max_), downsize=False)\n",
    "\n",
    "        trainloader = DataLoader(trainset, batch_size=train_batch_size, shuffle=True)\n",
    "        validloader = DataLoader(validset, batch_size=1,  shuffle=False)\n",
    "\n",
    "        LOG_loss_weights((trainset.labels==0).sum(), (trainset.labels==1).sum(), trainset.w_0, trainset.w_1, classifier_idx, fold_idx, path=out_path)\n",
    "        \n",
    "        # the dataset is unbalanced -> use weighted loss\n",
    "        loss_function = nn.CrossEntropyLoss(weight=torch.Tensor([trainset.w_0, trainset.w_1]).to(device))\n",
    "\n",
    "        print(f\"Computing fold {fold_idx:2d} mean and std...\")\n",
    "        if debug_mode:\n",
    "            mean, std = torch.Tensor([12.8338, 12.8575, 12.7818]), torch.Tensor([29.4589, 30.2996, 29.5237])\n",
    "            print(f\"skipping mean and std computation to speed up (DEBUG MODE ON). using mean: {mean}, std: {std}\")\n",
    "        else:\n",
    "            mean, std = calculate_mean_std(trainloader)\n",
    "            print(f\"Calculated mean: {mean}, std: {std}\")\n",
    "\n",
    "        LOG_norm(mean, std, classifier_idx, fold_idx, path=out_path)\n",
    "\n",
    "        normalize_transform = torchvision.transforms.Normalize(mean=mean, std=std)\n",
    "        \n",
    "        model, optimizer, scheduler = create_model(\n",
    "                                            ilr = initial_learning_rate,\n",
    "                                            scheduler_rate = scheduler_rate,\n",
    "                                            lr_last_fc_weight = lr_last_fc_weight\n",
    "                                        )\n",
    "        \n",
    "        LOG_training_hyperparams(initial_learning_rate, scheduler_rate, lr_last_fc_weight, train_batch_size, classifier_idx, fold_idx, path=out_path)\n",
    "\n",
    "        res = train_classifier(model, optimizer, scheduler, loss_function, normalize_transform, trainloader, validloader, max_epochs, classifier_idx, fold_idx, out_path, debug=debug_mode)\n",
    "        results.append(res)\n",
    "\n",
    "    avg_metrics = compute_avg_metrics(results)\n",
    "    print(\"Results for classifier\", classifier_idx); nice_print(avg_metrics)\n",
    "\n",
    "    LOG_metrics(avg_metrics, msg=f\"Results for classifier {classifier_idx}\", classifier_idx=classifier_idx, fold_idx=-1, path=out_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45bcbbd",
   "metadata": {},
   "source": [
    "## Train a single classifier on the full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b95582",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_results = []\n",
    "\n",
    "trainset = GenerativeDataset(fBBB, fold=None, mode=\"train\", normalization=None, downsize=False)\n",
    "validset = GenerativeDataset(fBBB, fold=None, mode=\"valid\", normalization=(trainset.min_, trainset.max_), downsize=False)\n",
    "\n",
    "trainloader = DataLoader(trainset, batch_size=train_batch_size, shuffle=True)\n",
    "validloader = DataLoader(validset, batch_size=1,  shuffle=False)\n",
    "\n",
    "# the dataset is unbalanced -> use weighted loss\n",
    "loss_function = nn.CrossEntropyLoss(weight=torch.Tensor([trainset.w_0, trainset.w_1]).to(device))\n",
    "\n",
    "print(f\"Computing fold {fold_idx:2d} mean and std...\")\n",
    "if debug_mode:\n",
    "    mean, std = torch.Tensor([12.8338, 12.8575, 12.7818]), torch.Tensor([29.4589, 30.2996, 29.5237])\n",
    "    print(f\"skipping mean and std computation to speed up (DEBUG MODE ON). using mean: {mean}, std: {std}\")\n",
    "else:\n",
    "    mean, std = calculate_mean_std(trainloader)\n",
    "    print(f\"Calculated mean: {mean}, std: {std}\")\n",
    "normalize_transform = torchvision.transforms.Normalize(mean=mean, std=std)\n",
    "\n",
    "for classifier_idx in range(classifiers_num):\n",
    "    print(colored(f\"Training classifier {classifier_idx+1}/{classifiers_num}\", \"yellow\"))\n",
    "    LOG_loss_weights((trainset.labels==0).sum(), (trainset.labels==1).sum(), trainset.w_0, trainset.w_1, classifier_idx, fold_idx=None, path=out_path)\n",
    "    LOG_norm(mean, std, classifier_idx, fold_idx=None, path=out_path)\n",
    "    \n",
    "    model, optimizer, scheduler = create_model(\n",
    "                                        ilr = initial_learning_rate,\n",
    "                                        scheduler_rate = scheduler_rate,\n",
    "                                        lr_last_fc_weight = lr_last_fc_weight\n",
    "                                    )\n",
    "    \n",
    "    LOG_training_hyperparams(initial_learning_rate, scheduler_rate, lr_last_fc_weight, train_batch_size, classifier_idx, fold_idx=None, path=out_path)\n",
    "\n",
    "    res = train_classifier(model, optimizer, scheduler, loss_function, normalize_transform, trainloader, validloader, max_epochs, classifier_idx, fold_idx=None, path=out_path, debug=debug_mode)\n",
    "    avg_results.append(res)\n",
    "    \n",
    "    \n",
    "avg_metrics = compute_avg_metrics(avg_results)\n",
    "print(colored(f\"Results for classifier {classifier_idx}\", \"yellow\")); nice_print(avg_metrics)\n",
    "\n",
    "avg_results_valid = [c_res[\"best_metrics_valid\"] for c_res in avg_results]\n",
    "\n",
    "print(colored(\"Best results among classifiers\", \"yellow\"))\n",
    "\n",
    "best_classifiers, best_values = compute_best_classifier(avg_results_valid)\n",
    "\n",
    "for key, classifier_idx in best_classifiers.items():\n",
    "    if key == \"loss\":\n",
    "        print(f\"Best classifier for {key} is classifier {classifier_idx} with value {best_values[key]:.4f}\")\n",
    "    else:\n",
    "        print(f\"Best classifier for {key} is classifier {classifier_idx} with value {best_values[key]*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b5c7d9",
   "metadata": {},
   "source": [
    "## Evaluate the ensemble on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16270681-79f6-4bc2-ac13-e97ef6012597",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_results = []\n",
    "all_logits = []\n",
    "trainset = GenerativeDataset(fBBB, fold=None, mode=\"train\", normalization=None, downsize=False)\n",
    "trainloader = DataLoader(trainset, batch_size=train_batch_size,  shuffle=True)\n",
    "\n",
    "print(f\"Computing fold {fold_idx:2d} mean and std...\")\n",
    "if debug_mode:\n",
    "    mean, std = torch.Tensor([12.8338, 12.8575, 12.7818]), torch.Tensor([29.4589, 30.2996, 29.5237])\n",
    "    print(f\"skipping mean and std computation to speed up (DEBUG MODE ON). using mean: {mean}, std: {std}\")\n",
    "else:\n",
    "    mean, std = calculate_mean_std(trainloader)\n",
    "    print(f\"Calculated mean: {mean}, std: {std}\")\n",
    "\n",
    "testset    = GenerativeDataset(fBBB, fold=None, mode=\"test\", normalization=(trainset.min_, trainset.max_), downsize=False)\n",
    "testloader = DataLoader(testset, batch_size=1,  shuffle=False)\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss(weight=torch.Tensor([trainset.w_0, trainset.w_1]).to(device))\n",
    "\n",
    "for classifier_idx in range(classifiers_num):\n",
    "    model, optimizer, _ = create_model(\n",
    "                                ilr = initial_learning_rate,\n",
    "                                scheduler_rate = scheduler_rate,\n",
    "                                lr_last_fc_weight = lr_last_fc_weight\n",
    "                            )\n",
    "    model, _, _, _ = load_model(model, optimizer, classifier_idx, fold_idx=None, path=out_path)\n",
    "    res, logits = test_classifier(model, loss_function, testloader, normalize_transform)\n",
    "    \n",
    "    print(colored(f\"Results for classifier {classifier_idx}\", \"yellow\")); nice_print(res)\n",
    "\n",
    "    avg_results.append(res)\n",
    "    all_logits.append(logits)\n",
    "  \n",
    "avg_metrics = compute_avg_metrics(avg_results)\n",
    "\n",
    "print(colored(f\"average test results\", \"yellow\")); nice_print(avg_metrics)\n",
    "\n",
    "best_classifiers, best_values = compute_best_classifier(avg_results)\n",
    "\n",
    "for key, classifier_idx in best_classifiers.items():\n",
    "    if key == \"loss\":\n",
    "        print(f\"Best classifier for {key} is classifier {classifier_idx} with value {best_values[key]:.4f}\")\n",
    "    else:\n",
    "        print(f\"Best classifier for {key} is classifier {classifier_idx} with value {best_values[key]*100:.2f}%\")\n",
    "\n",
    "\n",
    "\n",
    "## EVALUATE ENSEMBLE MODEL\n",
    "all_logits = np.array(all_logits)\n",
    "\n",
    "# Combine logits using the sum rule\n",
    "ensemble_logits = torch.Tensor(np.sum(all_logits, axis=0))\n",
    "ensemble_pred = (torch.nn.functional.softmax(ensemble_logits, dim=1).argmax(dim=1).int().numpy()).tolist()\n",
    "ensemble_gt = testset.labels\n",
    "\n",
    "# Compute metrics for the ensemble model\n",
    "ensemble_metrics = compute_metrics(ensemble_pred, ensemble_gt)\n",
    "print(colored(\"Ensemble model results\", \"yellow\"))\n",
    "nice_print(ensemble_metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-env",
   "language": "python",
   "name": "my-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
